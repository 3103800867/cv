{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《用纯Python手搓经典计算机视觉算法》开源教材 · 小作业2 —— Softmax 线性分类器\n",
    "\n",
    "> **版本号：v0.1**\n",
    ">\n",
    "> 前言：本篇章是山东大学（威海）2023级数科班nailorch小组大作业第二阶段展示内容，小组成员：魏子钦，刘艺航，刘宇，李暄，钦浩然。本章内容围绕深度学习线性分类器中的softmax分类器展开，本文代码所用编程语言为纯python，不含任何框架，简单易上手，希望能给刚入门的小白一点启发。\n",
    "\n",
    "# 0.导言\n",
    "在生活中，我们通常会遇到许多分类问题。我们对分类问题感兴趣：不是问“多少”，而是问“哪一个”：\n",
    "\n",
    "* 某个电子邮件是否属于垃圾邮件文件夹？\n",
    "\n",
    "* 某个用户可能注册或不注册订阅服务？\n",
    "\n",
    "* 某个图像描绘的是驴、狗、猫、还是鸡？\n",
    "\n",
    "* 某人接下来最有可能看哪部电影？\n",
    "\n",
    "Softmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard（硬）。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的**概率**。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。\n",
    "\n",
    "\n",
    "# 1. Softmax激活函数（Softmax Activation Function）\n",
    "\n",
    "单纯谈softmax，其实它就是一个函数，这个函数要有它的用的地方才能发挥它的作用，而我们通常会在神经网络的最后一层见到它。可能你有训练过神经网络来解决多类别分类（multiclass classification）问题，如果有，你一定知道，神经网络的原始输出往往很难解释。而Softmax 激活函数通过让神经网络的输出更易于理解为你解决了这一难题。Softmax 激活函数把神经网络的原始输出转换为一个概率向量，本质上是输入各个类别上的概率分布。考虑一个具有 N 个类别的多分类问题。Softmax 激活会返回一个长度为 N 的输出向量，其中索引为 i 的条目对应于某个输入属于第 i 类的概率。这里看不懂没关系，我们接着往下一点点拆解。\n",
    "\n",
    "\n",
    "## 1.1多分类问题\n",
    "\n",
    "回想一下，在二分类中，只有两种可能的类别，而在多分类中，则存在两种以上的可能类别。让我们考虑以下示例：给定一个包含熊猫、海豹和鸭子图像的数据集。你想训练一个分类器来预测一张之前未见过的图像是海豹、熊猫还是鸭子。请注意，下面的输入类别标签是如何进行独热编码的，并且这些类别是互斥的。在这种情况下，互斥性意味着给定图像一次只能是{海豹、熊猫、鸭子}中的一种。\n",
    "\n",
    "> 独热编码（One-Hot Encoding）：一种将分类变量转换为二进制向量的技术。它的主要目的是将离散的、无序的特征值转换为机器学习算法可以处理的格式。独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。例如，对于我们考虑的这个例子 ，独热编码后的结果为：海豹 => [1, 0, 0]；熊猫 => [0, 1, 0]；鸭子 => [0, 0, 1]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/2d1e97e5ba2540339d16ecdfc919f8750027e0d6a5f04788b8c77f2bd98eba46\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">多类分类示例</span>\n",
    "</p>\n",
    "\n",
    "## 1.2  Sigmoid与Argmax \n",
    "### 1.2.1 Sigmoid函数的局限性\n",
    "提到二分类首先想到的可能就是逻辑回归算法。逻辑回归算法是在各个领域中应用比较广泛的机器学习算法。逻辑回归算法本身并不难，最关键的步骤就是将线性模型输出的实数域映射到[0, 1]表示概率分布的有效实数空间，其中Sigmoid函数刚好具有这样的功能从数学上讲，Sigmoid激活函数由以下等式给出，它将所有输入压缩到范围 [0, 1] 内：\n",
    "\n",
    "$$ \n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z} \n",
    "$$\n",
    "$$ \n",
    "\\sigma(z) \\to 0 \\text{ as } z \\to -\\infty \n",
    "$$\n",
    "$$ \n",
    "\\sigma(z) \\to 1 \\text{ as } z \\to \\infty \n",
    "$$\n",
    "\n",
    "\n",
    "Sigmoid函数将任意实数作为输入，并将其映射到 0 到 1 之间的数字。这正是它非常适合二元分类的原因。\n",
    "\n",
    "▶️ 您可以运行以下代码来绘制一系列数字上的Sigmoid函数值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T13:44:06.724188Z",
     "iopub.status.busy": "2025-10-24T13:44:06.723827Z",
     "iopub.status.idle": "2025-10-24T13:44:07.230461Z",
     "shell.execute_reply": "2025-10-24T13:44:07.229906Z",
     "shell.execute_reply.started": "2025-10-24T13:44:06.724166Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid 函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 生成一系列输入（从 -10 到 10，共 200 个点）\n",
    "z = np.linspace(-10, 10, 200)\n",
    "s = sigmoid(z)\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(z, s, label=r'$\\sigma(z) = \\frac{1}{1 + e^{-z}}$')\n",
    "plt.title(\"Sigmoid Function Curve\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"σ(z)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们回到之前的例子，判断输入图像是否为熊猫。在这个例子中，令 z 为神经网络的原始输出。如果 σ(z) 表示给定图像属于第 1 类（熊猫）的概率，那么 1 - σ(z) 表示给定图像不属于第 1 类且不是熊猫的概率。你可以将 σ(z) 视为一个概率分数。然而，当你有两个以上的类别时，这种方法就行不通了.\n",
    "\n",
    "在多分类任务中，若对每个类别都使用 Sigmoidde得出：\n",
    "$$\n",
    "\\sigma(z_1), \\sigma(z_2), \\sigma(z_3)\n",
    "$$\n",
    "它们是彼此独立的，并 **不满足概率和为 1**，比如，结果可能出现：\n",
    "$$\n",
    "\\sigma(z_1)=0.9,\\;\\sigma(z_2)=0.8,\\;\\sigma(z_3)=0.7\n",
    "$$\n",
    "显然无法用概率解释，也无法仅通过阈值区分唯一类别。因此，Sigmoid **不适合多分类**。\n",
    "\n",
    "Softmax 则通过：\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_{j=1}^{N} e^{z_j}}\n",
    "$$\n",
    "强制所有类别的概率和为 1，使类别间产生竞争关系，更适合作为多分类任务的输出激活函数。事实上，你可以把softmax函数看作是sigmoid激活函数的向量泛化——当N = 2时，softmax和sigmoid激活函数是等价的，我们可以用一个二维输出向量 $\\mathbf{z} = [z_0, z_1]$ 来说明这一点。Softmax 的定义为：\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_k = \\frac{e^{z_k}}{e^{z_0} + e^{z_1}}\n",
    "$$\n",
    "\n",
    "对于第 0 类：\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_0\n",
    "= \\frac{e^{z_0}}{e^{z_0} + e^{z_1}}\n",
    "$$\n",
    "\n",
    "令 $(z = z_0 - z_1)$，可得：\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_0\n",
    "= \\frac{e^{z}}{1 + e^{z}}\n",
    "= \\frac{1}{1 + e^{-z}}\n",
    "= \\sigma(z)\n",
    "$$\n",
    "\n",
    "同理，对第 1 类：\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_1\n",
    "= \\frac{e^{z_1}}{e^{z_0} + e^{z_1}}\n",
    "= \\frac{1}{1 + e^{z}}\n",
    "= 1 - \\sigma(z)\n",
    "$$\n",
    "\n",
    "观察在这种情况下 softmax 激活分数与 sigmoid 激活分数相同：σ(z) 和 1 - σ(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Argmax 函数的局限性\n",
    "argmax 函数返回输入数组中最大值的索引。假设神经网络的原始输出向量为z = [0.25, 1.23, -0.8]。在这种情况下，最大值为 1.23，位于索引 1 处。在我们的图像分类示例中，索引 1 对应于第二类，并且该图像被预测为熊猫。在向量表示法中，最大值出现的索引处（向量z的索引为 1 ）为 1。所有其他索引处均为 0。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/f2325964ef954f9097163bf392df7e8213b624ecb7f340a69e4e829f6c1df1b8\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">Argmax 输出</span>\n",
    "</p>\n",
    "\n",
    "使用 argmax 函数的一个限制是，它相对于神经网络原始输出的梯度始终为零。众所周知，梯度的反向传播促进了神经网络的学习过程。由于在反向传播过程中，你必须将 argmax 输出的所有梯度代入 0 值，所以你无法在训练中使用 argmax 函数。除非进行梯度反向传播，否则神经网络的参数无法调整，实际上也就没有学习。从概率的角度来看，请注意 argmax 函数如何将所有质量放在索引 1（预测类别）上，而其他位置则为 0。因此，从 argmax 输出中推断预测类别标签非常简单。然而，我们想知道图像是熊猫、海豹或鸭子的可能性有多大，这种“软性”需求是argmax所不能达到的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Softmax是什么\n",
    "\n",
    "softmax激活函数接收一个神经网络原始输出向量作为输入，并返回一个概率分数向量。softmax函数的方程定义如下：\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\mathbf{z})_i\n",
    "= \\frac{e^{z_i}}{\\displaystyle \\sum_{j=1}^{N} e^{z_j}}\n",
    "$$\n",
    "\n",
    "其中，\n",
    "$\n",
    "\\mathbf{z} \n",
    "$\n",
    "表示输入向量；e是自然常数，数值约为2.718\n",
    "\n",
    "回想一下，在我们的例子中，N = 3，因为我们有 3 个类：{海豹、熊猫、鸭子}，有效索引为 0、1 和 2。假设给定神经网络原始输出的向量z = [0.25, 1.23, -0.8]。让我们将 softmax 公式应用于向量z，使用以下步骤：①计算每个条目的指数②将步骤 1 的结果除以所有条目的指数之和。\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/5e41f333a53a4551a2befebc5dd980deadd179ce6c764a6fb236abd2267fe6b9\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">计算3个类别的softmax分数</span>\n",
    "</p>\n",
    "\n",
    "现在我们已经计算出了 softmax 分数，让我们将它们收集到一个向量中以进行简洁的表示，如下所示：\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/3c891c8ab6cc497f8293e7c74fa9faaff41d9b0fe8c74c7b96352a2090243ca1\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">Softmax输出</span>\n",
    "</p>\n",
    "\n",
    "从上面的softmax输出，我们可以得出以下观察结果：\n",
    "\n",
    "* 在原始输出向量z中，最大值为 1.23，应用 softmax 激活函数后，其值映射到 0.664：softmax 输出向量中的最大项。同样，0.25 和 -0.8 分别映射到 0.249 和 0.087：softmax 输出向量中第二大和第三大项。因此，应用 softmax 函数可以保留分数的相对顺序。\n",
    "* softmax 输出向量都在 0 和 1 之间。\n",
    "* 在多类分类问题中，类别是互斥的，请注意 softmax 输出的条目总和为1：0.664 + 0.249 + 0.087 = 1。\n",
    "\n",
    "这就是为什么我们可以将softmax输出视为输入类别的概率分布，这使得它易于解释。\n",
    "\n",
    "在向量 softmax( z ) = [0.664, 0.294, 0.087] 中，索引 1 处的 0.664 是最大值。这意味着给定图像属于类别 1 的概率为 66.4%，而根据我们的独热编码，该类别属于熊猫类。输入图像有 29.4% 的可能性是海豹，有 8.7% 的可能性是鸭子。因此，应用softmax可以立即提供可解释性，因为我们可以知道测试图像属于这三个类别的可能性。在这个特定的例子中，它很可能是熊猫，而鸭子的可能性最小。现在，在 softmax 输出上调用 argmax 函数来获取预测的类标签就变得有意义了。由于预测的类标签是概率得分最高的标签，因此您可以使用它argmax(softmax(z))来获取预测的类标签。在我们的示例中，最高概率得分 0.664 出现在索引 1 处，对应于类 1（熊猫）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.损失函数\n",
    "## 2.1 全连接层网络架构\n",
    "> 仿射函数：仿射函数是由一阶多项式构成的函数，通常表示为 f(x) = Ax + b，其中 A 是一个矩阵，x 是一个向量，b 是一个向量。仿射函数反映了一种从k维到m维的空间映射关系，主要用于维度改变或形状、方向的改变，这个过程称为仿射变换。仿射函数是线性函数的一种推广，具有广泛的应用。\n",
    "\n",
    "为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，我们需要和输出一样多的**仿射函数（affine function）**。 每个输出对应于它自己的仿射函数。在我们的例子中，假设每次输入是一个2×2的灰度图像(假如输入一个熊猫的图像，我们把它分成四块，竖一刀横一刀)，我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_1$,$x_2$,$x_3$,$x_4$。由于我们有 4 个特征和 3 个可能的输出类别，我们将需要 12 个标量来表示权重（带下标的w），3 个标量来表示偏置（带下标的b）。下面我们为每个输入计算三个未规范化的预测（logit）：$o_1$、$o_2$和$o_3$。\n",
    "$$\n",
    "o_1 = x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\n",
    "$$\n",
    "$$\n",
    "o_2 = x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2, \n",
    "$$\n",
    "$$\n",
    "o_3 = x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "$$\n",
    "\n",
    "我们可以用下图来描述这个计算过程与线性回归一样，softmax回归也是一个单层神经网络。由于计算每个输出$o_1$、$o_2$和$o_3$取决于所有输入$x_1$,$x_2$,$x_3$,$x_4$，所以 softmax回归的输出层也是**全连接层**。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/5145f00cf885466aa6d4375423a578746d1fd086b8c84ce29d80cef82350052f\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">softmax回归是一种单层神经网络</span>\n",
    "</p>\n",
    "\n",
    ">全连接层（Fully Connected Layer），简称 FC 层，是人工神经网络中的基础层之一。最早应用于多层感知机（MLP），其功能是将输入数据的所有特征映射到输出层，进行分类或回归等任务。全连接层是神经网络中的最后一层，也被称为“密集连接层”。在全连接层中，输入的每个神经元都与输出的每个神经元相连接。全连接层通过对输入的线性变换和激活函数的非线性变换，将高维特征压缩或映射到目标维度。\n",
    "\n",
    "## 2.1附 全连接层的参数开销\n",
    "正如我们将在后续神经网络中会学到的一样，在深度学习中，全连接层无处不在。然而，顾名思义，全连接层是 “完全” 连接的，可能有很多可学习的参数。具体来说，对于任何具有d个输入和q个输出的全连接层，参数开销为$\\mathcal{O}(dq)$，这个数字在实践中可能高得令人望而却步。幸运的是，将d个输入转换为q个输出的成本可以减少到$\\mathcal{O}(\\frac{dq}{n})$，其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性.\n",
    "## 2.2 softmax求导\n",
    "现在可以构建比较复杂的神经网络模型，最重要的原因之一得益于**反向传播算法**。反向传播算法从输出端也就是损失函数开始向输入端基于链式法则计算梯度，然后通过计算得到的梯度，应用梯度下降算法迭代更新待优化参数。由于反向传播计算梯度基于**链式法则**，因此下面为了更加清晰，首先推导一下Softmax函数的导数。作为最后一层的激活函数，求导本身并不复杂，但是需要注意需要分成**两种情况**来考虑。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/e12312af3a3b435989c48cd95453f1ccec2ec044da1a49d4970f7c8d5f79db73\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">一个具体的例子</span>\n",
    "</p>\n",
    "\n",
    "我们以上图中具体例子来对softmax求导，绘制拥有三个输出节点的Softmax函数的计算图：\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/c175af0b5b8846879e3c1943985b20094591266aef524a5f98e5d75fb521c2f3\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">拥有三个输出节点的Softmax函数的计算图</span>\n",
    "</p>\n",
    "\n",
    "回顾 Softmax 函数的表达式：\n",
    "\n",
    "$$\n",
    "Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "其中 $i$ 表示输出节点的编号。\n",
    "\n",
    "影响 $y_1$ 的有与之相连的 $e^{z_1}, e^{z_2}, e^{z_3}$，因此需要分别求出 $\\frac{\\partial y_1}{\\partial z_1}$、$\\frac{\\partial y_1}{\\partial z_2}$、$\\frac{\\partial y_1}{\\partial z_3}$。此时输出值 $y_1$ 为\n",
    "\n",
    "$$\n",
    "y_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}\n",
    "$$\n",
    "\n",
    "很明显，$\\frac{\\partial y_1}{\\partial z_1}$ 与 $\\frac{\\partial y_1}{\\partial z_2}$、$\\frac{\\partial y_1}{\\partial z_3}$ 结果不同，而 $\\frac{\\partial y_1}{\\partial z_2}$、$\\frac{\\partial y_1}{\\partial z_3}$ 只需换相应索引即可。因此在对 Softmax 函数求导时，需要分两种情况考虑。即对第 $i$ 个输出节点，分别对 $j=i$ 的 $z_j$，求导以及其它 $j\\ne i$ 的 $z_j$ 求导。\n",
    "\n",
    "对 $j=i$ 时，类似前面介绍的 $\\frac{\\partial y_1}{\\partial z_1}$，Softmax 函数的偏导数可以展开为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{(e^{z_i})' \\sum_{c=1}^{C} e^{z_c} - e^{z_i} (\\sum_{c=1}^{C} e^{z_c})'}{(\\sum_{c=1}^{C} e^{z_c})^2}\n",
    "$$\n",
    "\n",
    "上面使用了函数相除的导数运算，由于是对 $z_j$ 求导，若此时 $j=i$，因此 $e^{z_i}$ 的导数还是 $e^{z_i}$ 本身，对 $\\sum_{c=1}^{C} e^{z_c}$ 求导结果只保留 $e^{z_j}$。因此上面求导的结果为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{e^{z_i} \\sum_{c=1}^{C} e^{z_c} - e^{z_i} e^{z_j}}{(\\sum_{c=1}^{C} e^{z_c})^2}\n",
    "$$\n",
    "\n",
    "提取公共项 $e^{z_i}$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{e^{z_i}(\\sum_{c=1}^{C} e^{z_c} - e^{z_j})}{(\\sum_{c=1}^{C} e^{z_c})^2}\n",
    "$$\n",
    "\n",
    "拆分成两部分：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\times \\frac{\\sum_{c=1}^{C} e^{z_c} - e^{z_j}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "为了方便，将 Softmax 函数表达式 $Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}$ 表示为 $p_i$，结果为\n",
    "\n",
    "$$\n",
    "p_i(1 - p_j)\n",
    "$$\n",
    "\n",
    "由于此时 $j=i$，则 $p_i=p_j$，因此最终结果为 $p_i - (p_i)^2$。\n",
    "\n",
    "对 $j\\ne i$ 时，类似前面介绍的 $\\frac{\\partial y_1}{\\partial z_2}$ 或 $\\frac{\\partial y_1}{\\partial z_3}$，Softmax 函数的偏导数 $\\frac{\\partial y_i}{\\partial z_j}$ 可以展开为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{(e^{z_i})' \\sum_{c=1}^{C} e^{z_c} - e^{z_i} (\\sum_{c=1}^{C} e^{z_c})'}{(\\sum_{c=1}^{C} e^{z_c})^2}\n",
    "$$\n",
    "\n",
    "上面使用了函数相除的导数运算，由于是对 $z_j$ 求导，若此时 $j\\ne i$，因此 $e^{z_i}$ 相当于常数，常数的导数为 0，对 $\\sum_{c=1}^{C} e^{z_c}$ 求导同样只保留 $e^{z_j}$。因此上面求导的结果为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= \\frac{0 - e^{z_i} e^{z_j}}{(\\sum_{c=1}^{C} e^{z_c})^2}\n",
    "$$\n",
    "\n",
    "分解两项相乘：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\left( \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\right)\n",
    "= -\\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}} \\times \\frac{e^{z_j}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "为了方便，将 Softmax 函数表达式 $Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}$ 表示为 $p_i$，结果为\n",
    "\n",
    "$$\n",
    "- p_j \\cdot p_i\n",
    "$$\n",
    "\n",
    "由于此时 $j\\ne i$，因此最终结果为$-p_j \\cdot p_i$。\n",
    "\n",
    "对于 Softmax 函数的梯度推导，所使用的是导数的基本运算，并不复杂。关键的是要对 $j=i$ 以及 $j\\ne i$ 两种情况分别讨论。偏导数的最终表达式如下：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "y_i (1 - y_i), & j=i \\\\\n",
    "- y_j \\cdot y_i, & j\\ne i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 对数似然与交叉熵函数（损失函数）\n",
    "接下来看看 Softmax 的损失函数。回顾 Softmax 函数的表达式：\n",
    "\n",
    "$$\n",
    "p_i = \\mathrm{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "其中 $i$ 表示输出节点的编号。\n",
    "\n",
    "### 2.3.1 对数化损失函数形式\n",
    "\n",
    "假设此时第 $i$ 个输出节点为正确类别对应的输出节点，则 $p_i$ 是正确类别对应输出节点的概率值。添加 $\\log$ 运算不会影响函数的单调性，首先对 $p_i$ 添加 $\\log$ 运算：\n",
    "\n",
    "$$\n",
    "\\log p_i = \\log \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "由于此时的 $p_i$ 是正确类别对应输出节点的概率，当然希望此时的 $p_i$ 越大越好（当然概率不能超过 1）。通常情况下不使用极值下降法来求解，因此只需要为 $\\log p_i$ 加上一个负号变成极小化的表达式，现在变成最小化函数，使得该函数越小越好，相当于最小化负对数似然：\n",
    "\n",
    "$$\n",
    "loss_i = -\\log p_i = -\\log \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "$$\n",
    "\n",
    "对上面的式子进一步处理：\n",
    "\n",
    "$$\n",
    "loss_i = -\\log p_i = -(\\,z_i - \\log \\sum_{c=1}^{C} e^{z_c}\\,)\n",
    "$$\n",
    "\n",
    "这样就将上面的 Softmax 一步一步转化成了 Softmax 的损失函数。\n",
    "\n",
    "### 2.3.2 传统交叉熵函数（损失函数）形式\n",
    "\n",
    "但**更通常我们说的交叉熵往往是下面的式子**：\n",
    "\n",
    "$$\n",
    "L = -\\sum_{c=1}^{C} y_c \\log(p_c)\n",
    "$$\n",
    "\n",
    ">**上面这种形式的损失函数和上面通过 Softmax 函数一步一步转变推导成的损失函数有什么区别呢？**\n",
    "\n",
    "### 2.3.3 两种形式等价性\n",
    "\n",
    "为了方便将第一个 $loss_i$ 命名为公式 1，将通常的交叉熵损失函数 $L$ 数命名为公式 2。其实公式 1 和公式 2 本质上是一样的，对公式 1 来说，只针对正确类别对应的输出节点值，将该位置的 Softmax 值取出来，而公式 2 则是直接将真实分布 $y$ 和预测分布 $p$ 的信息结合进行交叉熵的运算。\n",
    "\n",
    "对于分类问题而言真实标签 $y$ 一般是 one-hot 的形式。例如对于三分类来说，真实标签若为第 2 类，则 $y = [0,\\,1,\\,0]$，那么使用 one-hot 编码表示为 $[0,\\,1,\\,0]$，也就是说正确类别位置上的 $y_c$ 值为 1，而其它位置为 0。此时公式 2 展开为：\n",
    "\n",
    "$$\n",
    "L = -\\sum_{c=1}^{C} y_c \\log(p_c)\n",
    "= -(\\,1 \\cdot \\log(p_1) + 0 \\cdot \\log(p_0) + 0 \\cdot \\log(p_2)\\,)\n",
    "= -\\log(p_1)\n",
    "$$\n",
    "\n",
    "最终的结果为 $L=-1\\times \\log(p_1)$，公式 1 只是对正确类别位置计算损失值：\n",
    "\n",
    "$$\n",
    "loss_i = -\\log \\frac{e^{z_i}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    "= -\\log(p_1) = L\n",
    "$$\n",
    "\n",
    "### 2.3.4 交叉熵函数的梯度推导\n",
    "既然公式 1 和公式 2 两个损失函数一样，那么接下来计算损失函数的导数使用的也经常用公式 2。也就是\n",
    "\n",
    "$$\n",
    "L = -\\sum_{c=1}^{C} y_c \\log(p_c)\n",
    "$$\n",
    "\n",
    "在这里直接推导最终的损失函数 $L$ 对网络输出变量 $z_i$ 的偏导数，展开为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= -\\sum_{c=1}^{C} y_c \\frac{\\partial \\log(p_c)}{\\partial z_i}\n",
    "= -\\sum_{c=1}^{C} y_c \\frac{1}{p_c}\\frac{\\partial p_c}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "接下来利用复合函数分解成：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= -\\sum_{c=1}^{C} y_c \\frac{1}{p_c} \\frac{\\partial p_c}{\\partial z_i}\n",
    "= -y_i \\frac{1}{p_i}\\frac{\\partial p_i}{\\partial z_i}\n",
    "-\\sum_{c\\neq i} y_c \\frac{1}{p_c}\\frac{\\partial p_c}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "其中 $\\frac{\\partial p_c}{\\partial z_i}$ 就是我们前面推导的 Softmax 函数的偏导数。\n",
    "\n",
    "对于 Softmax 函数分为两种情况，因此需要将求和符号拆成 $c=i$ 以及 $c\\neq i$ 这两种情形。代入 $\\frac{\\partial p_c}{\\partial z_i}$，可得：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= -y_i(1-p_i)\n",
    "-\\sum_{c\\neq i} y_c \\frac{1}{p_c}(-p_c p_i)\n",
    "$$\n",
    "\n",
    "进一步化简为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= -y_i(1-p_i)\n",
    "+\\sum_{c\\neq i} y_c p_i\n",
    "$$\n",
    "\n",
    "至此完成了对交叉熵函数的梯度推导。对于分类问题中标签 $y$ 通过 one-hot 编码的方式，则有如下关系：\n",
    "\n",
    "$$\n",
    "\\sum_{c=1}^{C} y_c = 1,\\quad y_i + \\sum_{c\\neq i} y_c = 1\n",
    "$$\n",
    "\n",
    "因此将交叉熵的偏导数进一步简化为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= p_i - y_i\n",
    "$$\n",
    "\n",
    "虽然求导过程非常庞杂，但是最终推导的结果非常简单。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 重新审视交叉熵\n",
    "如果把熵 $H(P)$ 想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？交叉熵从 $P$ 到 $Q$，记为 $H(P, Q)$。我们可以把交叉熵想象为“主观概率为 $Q$ 的观察者在看到根据概率 $P$ 生成的数据时的预期惊异”。当 $P = Q$ 时，交叉熵达到最低。在这种情况下，从 $P$ 到 $Q$ 的交叉熵是 $H(P, P) = H(P)$。\n",
    "\n",
    "简而言之，我们可以从两方面来考虑交叉熵分类目标：\n",
    "\n",
    "（i）最大化观测数据的似然；\n",
    "\n",
    "（ii）最小化传达标签所需的惊异。\n",
    "\n",
    "## 2.5 Softmax-with-Loss层计算\n",
    ">注：在实际应用中，一般softmax解决的是分类问题,被他会把值规范到0到1之间，可以理解成一种概率，交叉熵就是来判断这个输出的概率和真实的值之间的一种损失，如果越小的话，说明计算的越准确。所以就可以用交叉熵维损失函数反向传导来更新参数，让目标值与真实值的损失尽量少，也就是训练的时候尽量的准确，而那些参数就是线性组合的w和b\n",
    "\n",
    "Softmax函数称为softmax层，交叉熵误差称为Cross Entropy Error层，两者的组合称为Softmax-with-Loss层。Softmax with Loss层的计算图如下。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/b9005e8792fb49d0a28e0f69880f4c434e69d140605946de9937a8a6906d6875\" width=\"800\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">Softmax-with-Loss层的计算图</span>\n",
    "</p>\n",
    "可以看到，Softmax-with-Loss层有些复杂。这里只给出了最终结果，对Softmax-with-Loss层的导出过程感兴趣的读者，请关注文章后续内容。Softmax-with-Loss层的计算图可以简化成下面的版本，还是以我们一开始举的三个动物为例。简化版本的计算图中，softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层。这里假设要进行3类分类，从前面的层接收3个输入（得分）。如图5-30所示，Softmax层将输入（a1, a2, a3）正规化，输出（y1, y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和动物标签（t1, t2, t3），从这些数据中输出损失L。\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecadee8bc2294248aa80975819bce785d513cc4f437b42539bcc1280a295c871\" width=\"700\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">简化版Softmax-with-Loss层的计算图</span>\n",
    "</p>\n",
    "\n",
    "Softmax层的反向传播得到了（y1 −t1,y2 − t2,y3 − t3）这样“漂亮”的结果。由于（y1,y2,y3）是 Softmax层的输出，（t1,t2,t3）是监督数据，所以（y1−t1,y2−t2,y3−t3）是 Softmax层的输出和动物标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax的输出）接近动物标签。因此，必须将神经网络的输出与动物标签的误差高效地传递给前面的层。刚刚的（y1−t1,y2−t2,y3−t3）正是 Softmax层的输出与动物标签的差，直截了当地表示了当前神经网络的输出与动物标签的误差。这里考虑一个具体的例子，比如思考动物标签是（0,1,0）， Softmax层的输出是(0.3,0.2,0.5)的情形。因为正确解标签处的概率是0.2（20%），这个时候的神经网络未能进行正确的识别。此时，Softmax层的反向传播传递的是(0.3,−0.8,0.5)这样一个大的误差。因为这个大的误差会向前面的层传播，所以Softmax层前面的层会从这个大的误差中学习到“大”的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.从零实现softmax分类器\n",
    "\n",
    "\n",
    "Softmax 分类器本质上是一个 **多分类逻辑回归（Multinomial Logistic Regression）** 模型。\n",
    "\n",
    "整体结构如下：\n",
    "\n",
    "```\n",
    "输入 X → 线性打分 z = XW + b \n",
    "       ↓\n",
    "  Softmax → 得到概率 p\n",
    "       ↓\n",
    "  计算损失 L\n",
    "       ↓\n",
    "  反向传播求梯度 (dW, db)\n",
    "       ↓\n",
    "  更新参数 W, b\n",
    "       ↓\n",
    "  下一轮训练\n",
    "```\n",
    "\n",
    "## 3.1 模型结构\n",
    "\n",
    "输入特征向量为：\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Softmax 分类器的线性打分为：\n",
    "\n",
    "$$\n",
    "z = xW + b\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $W \\in \\mathbb{R}^{d \\times K}$：权重矩阵  \n",
    "- $b \\in \\mathbb{R}^{1 \\times K}$：偏置项  \n",
    "- K ：类别数量\n",
    "\n",
    "以下是初始化方法：\n",
    "\n",
    "```python\n",
    "class NumpySoftmax:\n",
    "    def __init__(self, input_dim, num_classes, lr=0.1, reg=1e-4):\n",
    "        self.W = 0.001 * np.random.randn(input_dim, num_classes)\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "```\n",
    "\n",
    "\n",
    "## 3.2 Softmax 函数\n",
    "\n",
    "Softmax 将线性得分z转换为类别概率：\n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "为了防止数值溢出，通常会减去最大值：\n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{z_k - \\max(z)}}{\\sum_{j=1}^{K} e^{z_j - \\max(z)}}\n",
    "$$\n",
    "\n",
    "这样在数值上更稳定（具体的在下面的实验中验证）\n",
    "\n",
    "具体实现：\n",
    "\n",
    "```python\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)  # 防止溢出\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "## 3.3 损失函数（交叉熵 + 正则化）\n",
    "\n",
    "对于第 i 个样本的真实标签 $y_i$，交叉熵损失定义为：\n",
    "\n",
    "$$\n",
    "L_i = -\\log(p_{i, y_i})\n",
    "$$\n",
    "\n",
    "对所有样本取平均，并加入 L2 正则项：\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^{N} \\log(p_{i, y_i}) + \\frac{\\lambda}{2}|W|^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- 第一项是分类误差；\n",
    "- 第二项是权重惩罚（防止过拟合）；\n",
    "- $\\lambda$ 为正则化系数。\n",
    "\n",
    "## 3.4 反向传播（梯度推导）\n",
    "\n",
    "Softmax 输出与交叉熵损失组合后，梯度有一个非常优雅的形式：\n",
    "\n",
    "**对得分 \\(z\\) 的梯度：**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = p - y_{\\text{one-hot}}\n",
    "$$\n",
    "\n",
    "其中 $y_{\\text{one-hot}}$ 是标签的独热编码（one-hot vector）。\n",
    "\n",
    "**对权重 \\(W\\) 和偏置 \\(b\\) 的梯度：**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T (p - y_{\\text{one-hot}}) + \\lambda W\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} (p_i - y_i)\n",
    "$$\n",
    "\n",
    "```python\n",
    "    def compute_loss_and_grads(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        scores = X.dot(self.W) + self.b\n",
    "        probs = self.softmax(scores)\n",
    "\n",
    "        # 损失函数\n",
    "        loss = -np.mean(np.log(probs[np.arange(num_samples), y])) + 0.5 * self.reg * np.sum(self.W**2)\n",
    "\n",
    "      \n",
    "```\n",
    "\n",
    "## 3.5 参数更新（梯度下降）\n",
    "\n",
    "Softmax 分类器使用 **随机梯度下降（SGD）** 或 **mini-batch SGD** 优化：\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中 $\\eta$ 为学习率。\n",
    "\n",
    "```python\n",
    "def compute_loss_and_grads(self, X, y):\n",
    "   \n",
    "        # 梯度\n",
    "        dscores = probs\n",
    "        dscores[np.arange(num_samples), y] -= 1\n",
    "        dscores /= num_samples\n",
    "        dW = X.T.dot(dscores) + self.reg * self.W\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        return loss, dW, db\n",
    "```\n",
    "\n",
    "## 3.6 训练与预测阶段\n",
    "\n",
    "训练：\n",
    "\n",
    "```python\n",
    "def train(self, X, y, epochs=100, batch_size=256):\n",
    "    n = X.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        losses = []\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "            loss, dW, db = self.compute_loss_and_grads(X_batch, y_batch)\n",
    "            self.W -= self.lr * dW\n",
    "            self.b -= self.lr * db\n",
    "            losses.append(loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, loss={np.mean(losses):.4f}\")\n",
    "```\n",
    "\n",
    "训练流程：\n",
    "\n",
    "1. 打乱样本顺序；\n",
    "2. 切分 batch；\n",
    "3. 前向传播 → 计算损失；\n",
    "4. 反向传播 → 计算梯度；\n",
    "5. 参数更新。\n",
    "\n",
    "预测：\n",
    "\n",
    "```\n",
    "def predict(self, X):\n",
    "    scores = X.dot(self.W) + self.b\n",
    "    probs = self.softmax(scores)\n",
    "    return np.argmax(probs, axis=1)\n",
    "```\n",
    "\n",
    "预测时：\n",
    "\n",
    "- 计算各类概率；\n",
    "- 选择概率最大的类别作为输出。\n",
    "\n",
    "## 3.7 总体实现代码\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "class NumpySoftmax:\n",
    "    def __init__(self, input_dim, num_classes, lr=0.1, reg=1e-4):\n",
    "        self.W = 0.001 * np.random.randn(input_dim, num_classes)\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)  # 防止溢出\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss_and_grads(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        scores = X.dot(self.W) + self.b\n",
    "        probs = self.softmax(scores)\n",
    "\n",
    "        # 损失函数\n",
    "        loss = -np.mean(np.log(probs[np.arange(num_samples), y])) + 0.5 * self.reg * np.sum(self.W**2)\n",
    "\n",
    "        # 梯度\n",
    "        dscores = probs\n",
    "        dscores[np.arange(num_samples), y] -= 1\n",
    "        dscores /= num_samples\n",
    "        dW = X.T.dot(dscores) + self.reg * self.W\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        return loss, dW, db\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=256):\n",
    "        n = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            losses = []\n",
    "            for i in range(0, n, batch_size):\n",
    "                batch_idx = indices[i:i+batch_size]\n",
    "                X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "                loss, dW, db = self.compute_loss_and_grads(X_batch, y_batch)\n",
    "                self.W -= self.lr * dW\n",
    "                self.b -= self.lr * db\n",
    "                losses.append(loss)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, loss={np.mean(losses):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = X.dot(self.W) + self.b\n",
    "        probs = self.softmax(scores)\n",
    "        return np.argmax(probs, axis=1)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 MNIST数据集验证\n",
    "\n",
    "## 4.1 模型与实现\n",
    "\n",
    "### 4.1.1 传统Softmax数值稳定性问题\n",
    "\n",
    "Softmax 函数的标准形式为 $S(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$，其中 $z_i$ 是模型的原始输出分数（logits）。\n",
    "\n",
    "该形式在计算上存在一个严重缺陷：**数值溢出 (Overflow)**。在深度学习中，模型的原始输出 $z_i$ 可能非常大（例如 1000）或非常小（例如 -1000）。如果 $z_i$ 中的值很大，$e^{1000}$ 将是一个超出计算机浮点数表示范围的巨大数字，导致计算结果变为 $inf$ (无穷大)，后续的除法 $inf / inf$ 将得到 $NaN$ (Not a Number)，导致计算失败。\n",
    "\n",
    "为了解决这个问题，我们利用一个数学技巧：在分子和分母上同乘一个常数 $C$。\n",
    "\n",
    "$S(z_i) = \\frac{Ce^{z_i}}{\\sum_{j} Ce^{z_j}} = \\frac{e^{z_i + \\log C}}{\\sum_{j} e^{z_j + \\log C}}$\n",
    "\n",
    "我们巧妙地选择 $C = e^{-z_{\\max}}$，其中 $z_{\\max} = \\max(z_j)$ 是所有 $z$ 中的最大值。将 $\\log C = -z_{\\max}$ 代入，公式变为：\n",
    "\n",
    "$S(z_i) = \\frac{e^{z_i - z_{\\max}}}{\\sum_{j} e^{z_j - z_{\\max}}}$\n",
    "\n",
    "这个“减去最大值”的技巧确保了指数函数的最大输入为 $z_{\\max} - z_{\\max} = 0$，因此其输出 $e^0 = 1$。所有其他项 $e^{z_j - z_{\\max}}$ 都是 $e$ 的负数次幂（或 0），其值域稳定在 $(0, 1]$ 之间。这有效避免了上溢出问题，使得 Softmax 的计算在数值上保持稳定。此方法已在 `softmax.py` 脚本的 `softmax` 函数中实现。\n",
    "\n",
    "### 4.1.2 数据加载与预处理代码说明：\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "X_train_full = train_data.data.numpy().reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "y_train_full = train_data.targets.numpy()\n",
    "X_test = test_data.data.numpy().reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "y_test = test_data.targets.numpy()\n",
    "\n",
    "X_val, y_val = X_train_full[50000:], y_train_full[50000:]\n",
    "X_train, y_train = X_train_full[:50000], y_train_full[:50000]\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"验证集大小: {X_val.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n",
    "```\n",
    "\n",
    "这段代码负责加载和准备 MNIST 数据集：\n",
    "\n",
    "1.  **数据加载**：使用 `torchvision.datasets.MNIST` 下载并加载 MNIST 数据集。\n",
    "2.  **手动预处理**：\n",
    "      * `train_data.data.numpy()`：提取原始数据（像素值范围 $0$ - $255$）并转换为 NumPy 数组。\n",
    "      * `.reshape(-1, 28*28)`：将每张 $28 \\times 28$ 像素的图像“展平”为一个包含 784 个元素的一维向量。\n",
    "      * `.astype(\"float32\") / 255.0`：将数据类型转为浮点数，并将像素值从 $[0, 255]$ 区间归一化到 $[0.0, 1.0]$ 区间。这是模型实际使用的输入数据。\n",
    "3.  **数据集划分**：\n",
    "      * 原始的 60000 个训练样本被分为两部分：\n",
    "      * `X_train`, `y_train`：前 50000 个样本作为训练集。\n",
    "      * `X_val`, `y_val`：后 10000 个样本作为验证集，用于在训练过程中监控模型性能。\n",
    "      * `X_test`, `y_test`：10000 个独立的测试集样本，用于最终的模型评估。\n",
    "\n",
    "### 4.1.3 模型初始化与训练代码说明：\n",
    "\n",
    "```python\n",
    "# 1. 初始化模型\n",
    "clf = NumpySoftmax(input_dim=28*28, num_classes=10, lr=0.1, reg=1e-4)\n",
    "\n",
    "# 2. 训练模型 (使用100个epoch进行演示)\n",
    "history = clf.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=256)\n",
    "```\n",
    "\n",
    "这部分是模型训练的核心：\n",
    "\n",
    "1.  **初始化模型**：\n",
    "      * `clf = NumpySoftmax(...)`：创建了 `NumpySoftmax` 类的一个实例。\n",
    "      * `input_dim=28*28`：指定输入维度为 784，对应展平后的图像向量。\n",
    "      * `num_classes=10`：指定输出类别为 10 (数字 0 到 9)。\n",
    "      * `lr=0.1`：设置学习率 (Learning Rate) 为 0.1，这是梯度下降中更新权重的步长。\n",
    "      * `reg=1e-4`：设置 L2 正则化强度为 0.0001，用于防止模型过拟合。\n",
    "2.  **训练模型**：\n",
    "      * `clf.train(...)`：调用模型的训练方法。\n",
    "      * `X_train, y_train`：传入训练集数据和标签。\n",
    "      * `X_val, y_val`：传入验证集数据和标签，用于在每个轮次 (epoch) 结束时评估准确率。\n",
    "      * `epochs=100`：模型将完整遍历训练数据集 100 次。\n",
    "      * `batch_size=256`：在每个轮次中，数据被分成大小为 256 的小批量 (mini-batches) 进行处理和梯度更新。\n",
    "\n",
    "## 4.2 训练过程\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/9aa5f7f727b74371a2ca200f2bc12067fe752a1eaa0d4c41bb0a544fb9fdac66\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">训练过程</span>\n",
    "</p>\n",
    "\n",
    "* **左图 (训练损失曲线)**：显示了模型在训练集上的损失随轮次（Epoch）的变化。损失持续下降，表明模型在有效学习。\n",
    "* **右图 (验证集准确率曲线)**：显示了模型在验证集上的准确率随轮次的变化。准确率稳步提升并趋于平稳，表明模型收敛良好。\n",
    "\n",
    "## 4.3 模型评估\n",
    "\n",
    "### 4.3.1 总体准确率\n",
    "\n",
    "模型在 MNIST 测试集上的最终准确率为： **0.9227**\n",
    "\n",
    "### 4.3.2 详细分类报告\n",
    "\n",
    "| 类别 | 精确率 (Precision) | 召回率 (Recall) | F1 分数 | 支持数 (Support) |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| 0 | 0.9570 | 0.9776 | 0.9672 | 980 |\n",
    "| 1 | 0.9661 | 0.9789 | 0.9724 | 1135 |\n",
    "| 2 | 0.9270 | 0.8983 | 0.9124 | 1032 |\n",
    "| 3 | 0.9030 | 0.9129 | 0.9079 | 1010 |\n",
    "| 4 | 0.9286 | 0.9267 | 0.9276 | 982 |\n",
    "| 5 | 0.8936 | 0.8666 | 0.8799 | 892 |\n",
    "| 6 | 0.9351 | 0.9478 | 0.9414 | 958 |\n",
    "| 7 | 0.9259 | 0.9232 | 0.9245 | 1028 |\n",
    "| 8 | 0.8851 | 0.8778 | 0.8814 | 974 |\n",
    "| 9 | 0.8952 | 0.9058 | 0.9005 | 1009 |\n",
    "\n",
    "\n",
    "### 4.3.3 混淆矩阵\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/00a42df90cff4d3a89fee426f532c19cbd3890eb39284412913051bb1b0c231d\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">混淆矩阵</span>\n",
    "</p>\n",
    "\n",
    "* 混淆矩阵展示了模型预测的详细情况。\n",
    "* **行（Y轴）** 代表真实的标签。\n",
    "* **列（X轴）** 代表模型预测的标签。\n",
    "* 对角线上的数字表示预测正确的样本数量，颜色越深代表数量越多。\n",
    "* 非对角线上的数字（例如第4行第9列）表示有多少个真实的“4”被错误地预测为了“9”。\n",
    "\n",
    "## 4.4 重新审视模型实现\n",
    "\n",
    "### 4.4.1 学习到的权重\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/a6e6debc19b74fc3b1ed47a3e27ba7f2c44369291c2044ff89dad295db8e1d60\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">简化版Softmax-with-Loss层的计算图</span>\n",
    "</p>\n",
    "\n",
    "* 上图展示了模型为 10 个类别（数字 0 到 9）分别学习到的权重 $W$。\n",
    "* 每一张 $28 \\times 28$ 的图像代表一个类别的“模板”。\n",
    "* 高亮（颜色较亮）的区域表示模型认为对判断该数字**最重要**的像素。例如，数字“0”的模板在边缘是亮的，中间是暗的。\n",
    "\n",
    "\n",
    "### 4.4.2 错误样本分析\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ai-studio-static-online.cdn.bcebos.com/e5ed619527b1466da54bb374ee2000bebf4a937570d847f28b4553276b47896f\" width=\"600\">\n",
    "  <br>\n",
    "  <span style=\"font-size: 14px; color: gray;\">简化版Softmax-with-Loss层的计算图</span>\n",
    "</p>\n",
    "\n",
    "* 上图随机抽取了 10 个模型在测试集上预测错误的样本。\n",
    "* 每个样本下方标注了其 **真实标签** 和模型的 **错误预测**。\n",
    "* 这有助于我们直观地理解模型容易在哪些“模棱两可”的（或书写不清的）数字上犯错。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
